---
title:  M202第一周课程笔记
date:   2016-08-16
categories: MongoDB
---

根据[JOOQ](http://www.jooq.org/)对2015年数据库引擎的[分析报告](https://blog.jooq.org/2015/10/15/the-10-most-popular-db-engines-sql-and-nosql-in-2015)，MongoDB已经是最流行的NoSQL数据库。对于移动互联网开发者来说，MongoDB以它的简洁、快速和高效的特性在后台系统数据库选型时已经日益受到开发者的青睐。[MongoDB University](https://university.mongodb.com/)给广大开发者提供了一系列优质的在线培训课程，对小型创业团队来说确实是一件功德无量的事情。

在完成M101系列课程与M102之后，M202将大家带入到关于MongoDB在生产环境中实际应用时的高级运维话题，包括：

- 性能与监控
- 扩展性
- 故障应对
- 问题诊断

对于小型创业团队来说，配备专职的DBA过于奢侈，所以要求后台开发人员既要写好程序，也要做好运维。

## MongoDB的内存模型

在MongoDB版本3.0之前一直使用MMAPv1作为唯一的存储引擎，版本3.0引入WiredTiger（WT）存储引擎供用户选择，版本3.2开始将WT作为MongoDB的默认存储引擎。

### MMAPv1存储引擎

理解MMAPv1的工作机制需要掌握虚拟内存与mmap系统调用的基本概念，基本工作机制是将数据文件按页“惰性加载”（accessed lazily as needed）至物理内存，当物理内存被填满时通过LRU算法进行换页。


#### 工作集（Working Set）

MongoDB中工作集包含索引与数据子集（即热数据）两部分。

### 驻留内存（Resident Memory）

驻留内存是对一个进程实际占用内存大小的准确描述，我们可以通过Linux的`top`命令来查看一个进程的驻留内存大小：

![Resident Memory]({{ site.baseurl }}/images/m202/resident-memory.png)

截图中的`RES`即为进程占用的驻留内存信息。

#### 驻留内存与工作集的关系

驻留内存既可能远大于工作集也可能远小于工作集，不能通过查看mongod进程的驻留内存信息来估算工作集的大小。

驻留内存大于工作集的情况课程是这样解释的：

> It is just the difference between how recently and frequently that something has been accessed versus the fact that the process accessed at all, as therefor paged into memory at sometime in the past.

前者（recently and frequently that something has been accessed）是工作集，后者（process accessed at all）是驻留内存。简单来说，驻留内存会包含“冷数据”。

驻留内存小于工作集的情况与journalling有关。首先我们需要了解journaling的工作方式，见下图：

![Journaling]({{ site.baseurl }}/images/m202/journaling.png)

在完成remap之后，mongod将不再继续拥有原先Shared View（属于驻留内存）的存储空间，这块空间虽然被收归文件系统缓存（FS Cache）所有，但原先已装载的分页依然被保存在这块空间中，如果mongod需重新访问对应数据文件中的部分，文件系统将会快速将这些分页重新划归mongod的驻留内存。

### WT存储引擎

MongoDB 3.0起引入的WiredTiger存储引擎主要有以下特点：

- 文档级锁（MMAPv1为集合级锁）
- 使用BTree的方式存储文档（MMAPv1使用BTree存储索引而不是文档），从而无需再担心：
    * 文档移动（Document Moving）
    * 填充（Padding）
- 两份缓存
    * WT缓存（默认大小为1/2内存空间）
    * FS缓存：用于做Checkpoint（数据快照）——所以使用WT引擎之后无需再使用journaling
- 数据压缩

如果对文档移动与填充这两个概念有疑问的话，请复习M102课程。

## MongoDB与存储介质

### MongoDB与RAID

首先感性认识一下集中RAID类型

#### RAID0

借用MongoDB的术语来比喻的话，RAID0其实就是没有复制集的分片（即每个分片都是单点），好处是有多个并发的数据通道，加速访问速率，当然缺点也显而易见，当一个分片（磁盘）出故障时数据就因失去完整性而被破坏了，基本不在生产环境使用。

![RAID0]({{ site.baseurl }}/images/m202/RAID0.png)

#### RAID1

RAID1即镜像，在获得数据冗余的安全特性的同时代价也是显而易见的——可利用的存储空间减半。

![RAID1]({{ site.baseurl }}/images/m202/RAID1.png)

#### RAID10

RAID10是RAID0与RAID1的结合体，还是用MongoDB的属于来比喻，RAID10即使用复制集的方式支撑分片。推荐将RAID10应用于[EBS](https://aws.amazon.com/ebs/)。

![RAID10]({{ site.baseurl }}/images/m202/RAID1.png)

## 系统级调优（System Tuning）

系统级调优的内容很多，不可能面面俱到，关键是“纸上得来终觉浅，绝知此事要躬行”。当然有一些经验之谈还是值得借鉴。

### Readahead（预读）

Spinning Disk的寻址（Seek）是一个代价昂贵的过程，当定位到相应的数据需要按页读入时，内核一般会多读几K的数据，这多读的几K就是Readahead，如果访问的数据有比较好的连续性的话，后续的读取只会触发Soft Page Fault而不是Hard Page Falt，将极大的提高数据访问效率。

![Readahead]({{ site.baseurl }}/images/m202/Readahead.png)

Readahead严谨的定义是每一次磁盘访问额外读取的扇区（sector）数。我们可以通过`blockdev`命令来查看Readahead的大小。

![RA]({{ site.baseurl }}/images/m202/RA.png)

图中的`RA`是预读的扇区数，`SSZ`是一个扇区的消息，在该例中，预读的大小是`512B x 256 = 128KB`。预读对SSD存储的访存性能提升并不明显，原因很简单，SSD与Spinning Disk的机制完全不同。

## 第一周习题

### Homework 1.1: Readahead scenarios

Initially, your system has the following properties:

- You're working with a system where your indexes and part of your working set fit in memory
- You're not constrained by write locks
- You are using an SSD

Documents are typically sorted on one of several fields and order does not correspond to natural order, though often adjacent docs will be requested in the same query
You then change your system in the following ways, one at a time, before returning to your original state.

During which of the following changes should a higher readahead result in a larger performance increase than it would have for the initial system state?

Assume that any property of the system not mentioned in a particular choice is still in the state listed above.

- [*] You are now using a spinning disk, rather than an SSD.
- [*] You begin frequently accessing your data from capped collections, in the order in which it was written.
- [ ] You are now writing frequently, so that write locks become a constraint while reads have to wait.
- [ ] Your working set outgrows the available memory, so you are having to go to storage much more often.

解析：

- 选项1：Readahead对SSD存储没有意义，所以如果将SSD存储换成Spinning Disk，Readahead的效果就能体现出来
- 选项2：Capped collection是数据连续性的很好例子，非常贴合Readahead的应用场景
- 选项3：Readahead与写操作没有关系
- 选项4：如果连内存都没有了，那FS Cache（Readahead装载的目的地）也就不存在了

### Homework 1.2: Replica set chaining

You are operating a geographically dispersed replica set as outlined below:

![HW1.2]({{ site.baseurl }}/images/m202/replica_set_chaining.png)

Your network operations team has asked if you can limit the amount of outbound traffic from Site A because of some capacity issues with the traffic in and out of that site. For example, consider that this is where the majority of your users stream/download. However, due to how the application is deployed, you wish to keep your write traffic in Site A for performance, and so you have chosen not to change which server is primary.

Using replica set chaining, which of the following scenarios will minimize traffic due to MongoDB replication in and out of Site A?

Note: It will probably help you to draw each of the choices below for yourself as you think through them.

这个题太小白了 PASS

### Homework 1.3: Memory usage

You are performing an aggregation query with $sort, and are hitting the maximum size limit for in-memory sort. Which of the following might resolve this problem?

Hint: You will probably have to go to thedocumentationand consult aggregation to research the answer to this problem. This is intentional. Also you should also first rule out answers that are obviously incorrect.

- [*] Set the "allowDiskUse" parameter to true
- [ ] Switch out your HDD for an SSD so that data can be accessed more quickly
- [ ] Move your system to another machine with a faster CPU
- [*] Add an index for the variable(s) you are using to sort the documents
- [*] If you are not already doing so, include a $match earlier in the pipeline that will reduce the number of documents you are sorting

解析：

- 选项1：显而易见。有疑问的话查[官方文档](https://docs.mongodb.com/manual/reference/command/aggregate/)吧。
- 选项2：内存不够换硬盘也没有用啊
- 选项3：排序当然得用到CPU，但现在的问题是排序用的内存超出阈值，CPU再快也没有用
- 选项4：如果有索引就不需要将数据全部加载到内容中来排序，正解
- 选项5：减小排序的数据集大小，正解。这也是使用MongoDB任何时候都要有的一个好习惯，正如在查询时使用project只加载需要使用的数据域可以显小IO开销一样。
